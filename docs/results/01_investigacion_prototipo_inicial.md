# Investigaci贸n Inicial: An谩lisis del Prototipo de Bucle Completo

**Fecha:** 8 de Julio, 2025
**Versi贸n del Prototipo:** v0.1

## Resumen Ejecutivo

El objetivo de esta fase inicial era validar la arquitectura central del proyecto **Jarv1s**: un sistema de IA conversacional que opera de forma **100% local**, garantizando la privacidad del usuario. Se buscaba probar la viabilidad de integrar un pipeline completo de Speech-to-Text (STT), Large Language Model (LLM) y Text-to-Speech (TTS) utilizando un stack tecnol贸gico espec铆fico.

El resultado ha sido un **茅xito rotundo**. El prototipo funcional (`tests/test_full_loop.py`) ha demostrado ser capaz de mantener una conversaci贸n coherente y natural, validando las tecnolog铆as seleccionadas y estableciendo una base s贸lida para el desarrollo futuro.

## Stack Tecnol贸gico y Arquitectura del Prototipo

El prototipo se construy贸 siguiendo un flujo de datos modular, donde cada componente fue seleccionado estrat茅gicamente.

### 1. Entrada de Voz (STT): OpenAI Whisper

-   **Tecnolog铆a Utilizada:** `whisper`, el modelo de reconocimiento de voz de OpenAI.
-   **Modelo Espec铆fico:** `small`.
-   **Justificaci贸n de la Elecci贸n:** Aunque inicialmente se prob贸 el modelo `base`, se observaron imprecisiones. La migraci贸n al modelo `small` ofreci贸 un **salto cualitativo dr谩stico en la precisi贸n de la transcripci贸n**, incluso operando exclusivamente en **CPU**.
-   **Implementaci贸n Clave:** Para evitar conflictos de hardware (observados entre la inicializaci贸n de PyTorch y el driver de audio `sounddevice`), se adopt贸 una estrategia de **carga y descarga din谩mica**. El modelo Whisper no se mantiene en memoria, sino que se carga justo antes de la transcripci贸n y se libera inmediatamente despu茅s, garantizando la estabilidad del bucle.

### 2. Procesamiento Central (LLM): LM Studio + LiteLLM

-   **Tecnolog铆a Utilizada:** Un modelo de 1B de par谩metros servido localmente a trav茅s de **LM Studio**, con **LiteLLM** actuando como conector universal.
-   **Justificaci贸n de la Elecci贸n:** Esta combinaci贸n es el coraz贸n de la filosof铆a de Jarv1s.
    -   **LM Studio** abstrae la complejidad de la inferencia de modelos locales, proporcionando un endpoint compatible con la API de OpenAI.
    -   **LiteLLM** permite que nuestro c贸digo Python se comunique con este endpoint de forma estandarizada, d谩ndonos la flexibilidad de cambiar el servidor de LLM en el futuro sin reescribir el c贸digo de l贸gica.
-   **Implementaci贸n Clave (Memoria Conversacional):** El mayor logro en esta capa fue la implementaci贸n de un **historial de conversaci贸n**. La funci贸n `think` ahora mantiene una lista de los intercambios entre el usuario y el asistente. Este historial completo se env铆a al LLM en cada turno, lo que le permite entender el contexto y dar respuestas coherentes y relevantes, en lugar de tratar cada pregunta como un evento aislado.

### 3. Salida de Voz (TTS): Piper TTS

-   **Tecnolog铆a Utilizada:** `piper-tts`, un motor de s铆ntesis de voz r谩pido y de alta calidad.
-   **Modelo Espec铆fico:** `es_ES-sharvard-medium`.
-   **Justificaci贸n de la Elecci贸n:** Despu茅s de descartar `pyttsx3` por su baja calidad, **Piper** demostr贸 ser la soluci贸n ideal. Ofrece una voz natural y agradable con una latencia de s铆ntesis extremadamente baja, crucial para una conversaci贸n fluida y en tiempo real. Su rendimiento en CPU es excelente.

## An谩lisis de Resultados de la Conversaci贸n

La prueba de conversaci贸n completa arroj贸 resultados muy positivos que validan las decisiones de arquitectura:

#### Alta Precisi贸n en la Transcripci贸n:
Incluso con frases coloquiales, el modelo `small` de Whisper demostr贸 ser muy robusto.
> **T煤:** `Una C贸rmula de Matem谩ticas Bien Dif铆cil`
>
> **T煤:** `Est谩 interesante eso. Dime un chiste.`

#### Coherencia gracias a la Memoria:
El siguiente extracto demuestra perfectamente el 茅xito de la implementaci贸n del historial. Jarv1s recuerda el turno anterior y responde en contexto.
> **T煤:** `Est谩 interesante eso. Dime un chiste.`
>
> **Jarv1s:** `驴Por qu茅 los matem谩ticos prefieren las matem谩ticas a las personas? 隆Porque son m谩s estables! `
>
> **T煤:** `Ah me dio mucha gracia`
>
> **Jarv1s:** `隆Me alegro de que te haya gustado! 驴Quieres escuchar otro? `

Este comportamiento es imposible sin un historial de conversaci贸n, y su correcto funcionamiento es una de las mayores victorias de este prototipo.
